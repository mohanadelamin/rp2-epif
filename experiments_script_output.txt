Reading test variables from usecases_test_vars.csv
Running Test number 1
Creating Directory hpa_test_data/TEST_NO_1
Start Worker node monitoring
Deploying the Briding function, Proxy, and Server
"epi-helm" already exists with the same configuration, skipping
NAME: epi-bf
LAST DEPLOYED: Wed Jun 23 17:13:51 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
Deploying Locust load generator
"deliveryhero" already exists with the same configuration, skipping
configmap/loadtest-locustfile created
configmap/loadtest-lib created
NAME: locust
LAST DEPLOYED: Wed Jun 23 17:14:00 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
## To access the locust master UI
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace epi svc -w locust'
  export SERVICE_IP=$(kubectl get svc --namespace epi locust --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:
Setup is deploying, sleeping for 10 seconds
Start the HPA Monitoring script
Checking for Locust Serivce IP and Port
Locust service URL is http://145.100.110.82:8089
Checking if Locust service is ready.
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust retrun 200, Service is ready.
Locust is starting, sleeping for 10 seconds
Starting locust request
http://145.100.110.82:8089/swarm
<Response [200]>
progress: 0/100
progress: 1/100
progress: 2/100
progress: 3/100
progress: 4/100
progress: 5/100
progress: 6/100
progress: 7/100
progress: 8/100
progress: 9/100
progress: 10/100
progress: 11/100
progress: 12/100
progress: 13/100
progress: 14/100
progress: 15/100
progress: 16/100
progress: 17/100
progress: 18/100
progress: 19/100
progress: 20/100
progress: 21/100
progress: 22/100
progress: 23/100
progress: 24/100
progress: 25/100
progress: 26/100
progress: 27/100
progress: 28/100
progress: 29/100
progress: 30/100
progress: 31/100
progress: 32/100
progress: 33/100
progress: 34/100
progress: 35/100
progress: 36/100
progress: 37/100
progress: 38/100
progress: 39/100
progress: 40/100
progress: 41/100
progress: 42/100
progress: 43/100
progress: 44/100
progress: 45/100
progress: 46/100
progress: 47/100
progress: 48/100
progress: 49/100
progress: 50/100
progress: 51/100
progress: 52/100
progress: 53/100
progress: 54/100
progress: 55/100
progress: 56/100
progress: 57/100
progress: 58/100
progress: 59/100
progress: 60/100
progress: 61/100
progress: 62/100
progress: 63/100
progress: 64/100
progress: 65/100
progress: 66/100
progress: 67/100
progress: 68/100
progress: 69/100
progress: 70/100
progress: 71/100
progress: 72/100
progress: 73/100
progress: 74/100
progress: 75/100
progress: 76/100
progress: 77/100
progress: 78/100
progress: 79/100
progress: 80/100
progress: 81/100
progress: 82/100
progress: 83/100
progress: 84/100
progress: 85/100
progress: 86/100
progress: 87/100
progress: 88/100
progress: 89/100
progress: 90/100
progress: 91/100
progress: 92/100
progress: 93/100
progress: 94/100
progress: 95/100
progress: 96/100
progress: 97/100
progress: 98/100
progress: 99/100
Test done, Starting data collection after 5 seconds
Collecting Locust stats
Collecting worker response times file
tar: Removing leading `/' from member names
Killing the HPA monitoring script
./run_experiments.sh: line 39: 2126243 Killed                  bash scripts/hpa_monitor.sh ${TEST_DIR}
Cleaning up setup.
release "epi-bf" uninstalled
release "locust" uninstalled
configmap "loadtest-locustfile" deleted
configmap "loadtest-lib" deleted
Collecting briding function stats
Copying BF stats from 145.100.110.91 to hpa_test_data/TEST_NO_1
Copying BF stats from 145.100.110.92 to hpa_test_data/TEST_NO_1
Deleting old bridging function logs
Removing BF stats from 145.100.110.91
Removing BF stats from 145.100.110.92
Killing the Worker node monitoring script
./run_experiments.sh: line 39: 2125980 Killed                  sudo python3 scripts/xen_vm_stats.py ${TEST_DIR} > /dev/null 2> /dev/null
Test No. 1 is done, wait till all pods are terminated
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Running Test number 2
Creating Directory hpa_test_data/TEST_NO_2
Start Worker node monitoring
Deploying the Briding function, Proxy, and Server
"epi-helm" already exists with the same configuration, skipping
NAME: epi-bf
LAST DEPLOYED: Wed Jun 23 17:17:30 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
Deploying Locust load generator
"deliveryhero" already exists with the same configuration, skipping
configmap/loadtest-locustfile created
configmap/loadtest-lib created
NAME: locust
LAST DEPLOYED: Wed Jun 23 17:17:38 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
## To access the locust master UI
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace epi svc -w locust'
  export SERVICE_IP=$(kubectl get svc --namespace epi locust --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:
Setup is deploying, sleeping for 10 seconds
Start the HPA Monitoring script
Locust service URL is http://145.100.110.82:8089
Checking if Locust service is ready.
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust retrun 200, Service is ready.
Locust is starting, sleeping for 10 seconds
Starting locust request
http://145.100.110.82:8089/swarm
<Response [200]>
progress: 0/100
progress: 1/100
progress: 2/100
progress: 3/100
progress: 4/100
progress: 5/100
progress: 6/100
progress: 7/100
progress: 8/100
progress: 9/100
progress: 10/100
progress: 11/100
progress: 12/100
progress: 13/100
progress: 14/100
progress: 15/100
progress: 16/100
progress: 17/100
progress: 18/100
progress: 19/100
progress: 20/100
progress: 21/100
progress: 22/100
progress: 23/100
progress: 24/100
progress: 25/100
progress: 26/100
progress: 27/100
progress: 28/100
progress: 29/100
progress: 30/100
progress: 31/100
progress: 32/100
progress: 33/100
progress: 34/100
progress: 35/100
progress: 36/100
progress: 37/100
progress: 38/100
progress: 39/100
progress: 40/100
progress: 41/100
progress: 42/100
progress: 43/100
progress: 44/100
progress: 45/100
progress: 46/100
progress: 47/100
progress: 48/100
progress: 49/100
progress: 50/100
progress: 51/100
progress: 52/100
progress: 53/100
progress: 54/100
progress: 55/100
progress: 56/100
progress: 57/100
progress: 58/100
progress: 59/100
progress: 60/100
progress: 61/100
progress: 62/100
progress: 63/100
progress: 64/100
progress: 65/100
progress: 66/100
progress: 67/100
progress: 68/100
progress: 69/100
progress: 70/100
progress: 71/100
progress: 72/100
progress: 73/100
progress: 74/100
progress: 75/100
progress: 76/100
progress: 77/100
progress: 78/100
progress: 79/100
progress: 80/100
progress: 81/100
progress: 82/100
progress: 83/100
progress: 84/100
progress: 85/100
progress: 86/100
progress: 87/100
progress: 88/100
progress: 89/100
progress: 90/100
progress: 91/100
progress: 92/100
progress: 93/100
progress: 94/100
progress: 95/100
progress: 96/100
progress: 97/100
progress: 98/100
progress: 99/100
Test done, Starting data collection after 5 seconds
Collecting Locust stats
Collecting worker response times file
tar: Removing leading `/' from member names
Killing the HPA monitoring script
./run_experiments.sh: line 39: 2130313 Killed                  bash scripts/hpa_monitor.sh ${TEST_DIR}
Cleaning up setup.
release "epi-bf" uninstalled
release "locust" uninstalled
configmap "loadtest-locustfile" deleted
configmap "loadtest-lib" deleted
Collecting briding function stats
Copying BF stats from 145.100.110.91 to hpa_test_data/TEST_NO_2
Copying BF stats from 145.100.110.92 to hpa_test_data/TEST_NO_2
Deleting old bridging function logs
Removing BF stats from 145.100.110.91
Removing BF stats from 145.100.110.92
Killing the Worker node monitoring script
./run_experiments.sh: line 39: 2130126 Killed                  sudo python3 scripts/xen_vm_stats.py ${TEST_DIR} > /dev/null 2> /dev/null
Test No. 2 is done, wait till all pods are terminated
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Running Test number 3
Creating Directory hpa_test_data/TEST_NO_3
Start Worker node monitoring
Deploying the Briding function, Proxy, and Server
"epi-helm" already exists with the same configuration, skipping
NAME: epi-bf
LAST DEPLOYED: Wed Jun 23 17:21:12 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
Deploying Locust load generator
"deliveryhero" already exists with the same configuration, skipping
configmap/loadtest-locustfile created
configmap/loadtest-lib created
NAME: locust
LAST DEPLOYED: Wed Jun 23 17:21:22 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
## To access the locust master UI
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace epi svc -w locust'
  export SERVICE_IP=$(kubectl get svc --namespace epi locust --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:
Setup is deploying, sleeping for 10 seconds
Start the HPA Monitoring script
Locust service URL is http://145.100.110.82:8089
Checking if Locust service is ready.
Locust not ready yet, checking again!
Locust retrun 200, Service is ready.
Locust is starting, sleeping for 10 seconds
Starting locust request
http://145.100.110.82:8089/swarm
<Response [200]>
progress: 0/100
progress: 1/100
progress: 2/100
progress: 3/100
progress: 4/100
progress: 5/100
progress: 6/100
progress: 7/100
progress: 8/100
progress: 9/100
progress: 10/100
progress: 11/100
progress: 12/100
progress: 13/100
progress: 14/100
progress: 15/100
progress: 16/100
progress: 17/100
progress: 18/100
progress: 19/100
progress: 20/100
progress: 21/100
progress: 22/100
progress: 23/100
progress: 24/100
progress: 25/100
progress: 26/100
progress: 27/100
progress: 28/100
progress: 29/100
progress: 30/100
progress: 31/100
progress: 32/100
progress: 33/100
progress: 34/100
progress: 35/100
progress: 36/100
progress: 37/100
progress: 38/100
progress: 39/100
progress: 40/100
progress: 41/100
progress: 42/100
progress: 43/100
progress: 44/100
progress: 45/100
progress: 46/100
progress: 47/100
progress: 48/100
progress: 49/100
progress: 50/100
progress: 51/100
progress: 52/100
progress: 53/100
progress: 54/100
progress: 55/100
progress: 56/100
progress: 57/100
progress: 58/100
progress: 59/100
progress: 60/100
progress: 61/100
progress: 62/100
progress: 63/100
progress: 64/100
progress: 65/100
progress: 66/100
progress: 67/100
progress: 68/100
progress: 69/100
progress: 70/100
progress: 71/100
progress: 72/100
progress: 73/100
progress: 74/100
progress: 75/100
progress: 76/100
progress: 77/100
progress: 78/100
progress: 79/100
progress: 80/100
progress: 81/100
progress: 82/100
progress: 83/100
progress: 84/100
progress: 85/100
progress: 86/100
progress: 87/100
progress: 88/100
progress: 89/100
progress: 90/100
progress: 91/100
progress: 92/100
progress: 93/100
progress: 94/100
progress: 95/100
progress: 96/100
progress: 97/100
progress: 98/100
progress: 99/100
Test done, Starting data collection after 5 seconds
Collecting Locust stats
Collecting worker response times file
tar: Removing leading `/' from member names
Killing the HPA monitoring script
./run_experiments.sh: line 39: 2134508 Killed                  bash scripts/hpa_monitor.sh ${TEST_DIR}
Cleaning up setup.
release "epi-bf" uninstalled
release "locust" uninstalled
configmap "loadtest-locustfile" deleted
configmap "loadtest-lib" deleted
Collecting briding function stats
Copying BF stats from 145.100.110.91 to hpa_test_data/TEST_NO_3
Copying BF stats from 145.100.110.92 to hpa_test_data/TEST_NO_3
Deleting old bridging function logs
Removing BF stats from 145.100.110.91
Removing BF stats from 145.100.110.92
Killing the Worker node monitoring script
Test No. 3 is done, wait till all pods are terminated
Not all pods are terminated yet.
./run_experiments.sh: line 39: 2134302 Killed                  sudo python3 scripts/xen_vm_stats.py ${TEST_DIR} > /dev/null 2> /dev/null
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Running Test number 4
Creating Directory hpa_test_data/TEST_NO_4
Start Worker node monitoring
Deploying the Briding function, Proxy, and Server
"epi-helm" already exists with the same configuration, skipping
NAME: epi-bf
LAST DEPLOYED: Wed Jun 23 17:24:47 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
Deploying Locust load generator
"deliveryhero" already exists with the same configuration, skipping
configmap/loadtest-locustfile created
configmap/loadtest-lib created
NAME: locust
LAST DEPLOYED: Wed Jun 23 17:24:56 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
## To access the locust master UI
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace epi svc -w locust'
  export SERVICE_IP=$(kubectl get svc --namespace epi locust --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:
Setup is deploying, sleeping for 10 seconds
Start the HPA Monitoring script
Locust service URL is http://145.100.110.82:8089
Checking if Locust service is ready.
Locust not ready yet, checking again!
Locust retrun 200, Service is ready.
Locust is starting, sleeping for 10 seconds
Starting locust request
http://145.100.110.82:8089/swarm
<Response [200]>
progress: 0/100
progress: 1/100
progress: 2/100
progress: 3/100
progress: 4/100
progress: 5/100
progress: 6/100
progress: 7/100
progress: 8/100
progress: 9/100
progress: 10/100
progress: 11/100
progress: 12/100
progress: 13/100
progress: 14/100
progress: 15/100
progress: 16/100
progress: 17/100
progress: 18/100
progress: 19/100
progress: 20/100
progress: 21/100
progress: 22/100
progress: 23/100
progress: 24/100
progress: 25/100
progress: 26/100
progress: 27/100
progress: 28/100
progress: 29/100
progress: 30/100
progress: 31/100
progress: 32/100
progress: 33/100
progress: 34/100
progress: 35/100
progress: 36/100
progress: 37/100
progress: 38/100
progress: 39/100
progress: 40/100
progress: 41/100
progress: 42/100
progress: 43/100
progress: 44/100
progress: 45/100
progress: 46/100
progress: 47/100
progress: 48/100
progress: 49/100
progress: 50/100
progress: 51/100
progress: 52/100
progress: 53/100
progress: 54/100
progress: 55/100
progress: 56/100
progress: 57/100
progress: 58/100
progress: 59/100
progress: 60/100
progress: 61/100
progress: 62/100
progress: 63/100
progress: 64/100
progress: 65/100
progress: 66/100
progress: 67/100
progress: 68/100
progress: 69/100
progress: 70/100
progress: 71/100
progress: 72/100
progress: 73/100
progress: 74/100
progress: 75/100
progress: 76/100
progress: 77/100
progress: 78/100
progress: 79/100
progress: 80/100
progress: 81/100
progress: 82/100
progress: 83/100
progress: 84/100
progress: 85/100
progress: 86/100
progress: 87/100
progress: 88/100
progress: 89/100
progress: 90/100
progress: 91/100
progress: 92/100
progress: 93/100
progress: 94/100
progress: 95/100
progress: 96/100
progress: 97/100
progress: 98/100
progress: 99/100
Test done, Starting data collection after 5 seconds
Collecting Locust stats
Collecting worker response times file
tar: Removing leading `/' from member names
Killing the HPA monitoring script
./run_experiments.sh: line 39: 2138130 Killed                  bash scripts/hpa_monitor.sh ${TEST_DIR}
Cleaning up setup.
release "epi-bf" uninstalled
release "locust" uninstalled
configmap "loadtest-locustfile" deleted
configmap "loadtest-lib" deleted
Collecting briding function stats
Copying BF stats from 145.100.110.91 to hpa_test_data/TEST_NO_4
Copying BF stats from 145.100.110.92 to hpa_test_data/TEST_NO_4
Deleting old bridging function logs
Removing BF stats from 145.100.110.91
Removing BF stats from 145.100.110.92
Killing the Worker node monitoring script
Test No. 4 is done, wait till all pods are terminated
Not all pods are terminated yet.
./run_experiments.sh: line 39: 2137920 Killed                  sudo python3 scripts/xen_vm_stats.py ${TEST_DIR} > /dev/null 2> /dev/null
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Not all pods are terminated yet.
Running Test number 5
Creating Directory hpa_test_data/TEST_NO_5
Start Worker node monitoring
Deploying the Briding function, Proxy, and Server
"epi-helm" already exists with the same configuration, skipping
NAME: epi-bf
LAST DEPLOYED: Wed Jun 23 17:28:22 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
Deploying Locust load generator
"deliveryhero" already exists with the same configuration, skipping
configmap/loadtest-locustfile created
configmap/loadtest-lib created
NAME: locust
LAST DEPLOYED: Wed Jun 23 17:28:30 2021
NAMESPACE: epi
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
## To access the locust master UI
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace epi svc -w locust'
  export SERVICE_IP=$(kubectl get svc --namespace epi locust --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")
  echo http://$SERVICE_IP:
Setup is deploying, sleeping for 10 seconds
Start the HPA Monitoring script
Locust service URL is http://145.100.110.82:8089
Checking if Locust service is ready.
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust not ready yet, checking again!
Locust retrun 200, Service is ready.
Locust is starting, sleeping for 10 seconds
Starting locust request
http://145.100.110.82:8089/swarm
<Response [200]>
progress: 0/100
progress: 1/100
progress: 2/100
progress: 3/100
progress: 4/100
progress: 5/100
progress: 6/100
progress: 7/100
progress: 8/100
progress: 9/100
progress: 10/100
progress: 11/100
progress: 12/100
progress: 13/100
progress: 14/100
progress: 15/100
progress: 16/100
progress: 17/100
progress: 18/100
progress: 19/100
progress: 20/100
progress: 21/100
progress: 22/100
progress: 23/100
progress: 24/100
progress: 25/100
progress: 26/100
progress: 27/100
progress: 28/100
progress: 29/100
progress: 30/100
progress: 31/100
progress: 32/100
progress: 33/100
progress: 34/100
progress: 35/100
progress: 36/100
progress: 37/100
progress: 38/100
progress: 39/100
progress: 40/100
progress: 41/100
progress: 42/100
progress: 43/100
progress: 44/100
progress: 45/100
progress: 46/100
progress: 47/100
progress: 48/100
progress: 49/100
progress: 50/100
progress: 51/100
progress: 52/100
progress: 53/100
progress: 54/100
progress: 55/100
progress: 56/100
progress: 57/100
progress: 58/100
progress: 59/100
progress: 60/100
progress: 61/100
progress: 62/100
progress: 63/100
progress: 64/100
progress: 65/100
progress: 66/100
progress: 67/100
progress: 68/100
progress: 69/100
progress: 70/100
progress: 71/100
progress: 72/100
progress: 73/100
progress: 74/100
progress: 75/100
progress: 76/100
progress: 77/100
progress: 78/100
progress: 79/100
progress: 80/100
progress: 81/100
progress: 82/100
progress: 83/100
progress: 84/100
progress: 85/100
progress: 86/100
progress: 87/100
progress: 88/100
progress: 89/100
progress: 90/100
progress: 91/100
progress: 92/100
progress: 93/100
progress: 94/100
progress: 95/100
progress: 96/100
progress: 97/100
progress: 98/100
progress: 99/100
Test done, Starting data collection after 5 seconds
Collecting Locust stats
